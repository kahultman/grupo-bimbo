---
title: "Grupo Bimbo data analysis"
author: "Fabienvs"
date: "June 15, 2016"
output:
    html_document:
        toc: true
        highlight: zenburn
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=F, warning=F, fig.width=9.475, fig.height=5)
```

```{r}
library(readr)
library(dplyr)
library(ggplot2)
library(scales)
library(treemap)
```

```{r}
train <- read_csv("../input/train.csv")
#train <- train %>% sample_frac(0.001)
client <- read_csv("../input/cliente_tabla.csv")
product <- read_csv("../input/producto_tabla.csv")
town <- read_csv("../input/town_state.csv")
```

# Weeks
```{r}
ggplot(train %>% sample_frac(0.005))+
  geom_histogram(aes(x=Semana), color="black", fill="red", alpha=0.5)+
  scale_x_continuous(breaks=1:10)+
  scale_y_continuous(name="Client / Product deliveries")+
  theme_bw()
```

# Agencies & States
```{r}
agencias <- train %>%
  group_by(Agencia_ID) %>%
  summarise(Units = sum(Venta_uni_hoy),
            Pesos = sum(Venta_hoy),
            Return_Units = sum(Dev_uni_proxima),
            Return_Pesos = sum(Dev_proxima),
            Net = sum(Demanda_uni_equil)) %>%
  mutate(Net_Pesos = Pesos - Return_Pesos,
         Return_Rate = Return_Units / (Units+Return_Units)) %>%
  arrange(desc(Units)) %>%
  inner_join(town, by="Agencia_ID")
  
ggplot(agencias, aes(x=Units/7))+
  geom_histogram(fill="red", color="gray", binwidth=10000)+
  scale_x_continuous(name="Units / Week", labels=function(x)paste(x/1000, "k"))+
  scale_y_continuous(name="Agencias")+
  theme_bw()

treemap(agencias[1:100, ], 
        index=c("Agencia_ID"), vSize="Units", vColor="Return_Rate", 
        palette=c("#FFFFFF","#FFFFFF","#FF0000"),
        type="value", title.legend="Units return %", title="Top 100 agencias")
```

```{r echo=FALSE}
top30agencias <- agencias$Agencia_ID[1:30]
top100agencias <- agencias$Agencia_ID[1:100]
rm(agencias)
```

```{r, fig.height=8}
agencias.history <- train %>%
  group_by(Agencia_ID, Semana) %>%
  summarise(Units = sum(Venta_uni_hoy),
            Pesos = sum(Venta_hoy),
            Return_Units = sum(Dev_uni_proxima),
            Return_Pesos = sum(Dev_proxima),
            Net = sum(Demanda_uni_equil)) %>%
  mutate(Net_Pesos = Pesos - Return_Pesos,
         Avg_Pesos = Pesos / Units,
         Return_Rate = Return_Units / (Units+Return_Units)) %>%
  arrange(Agencia_ID, Semana) %>%
  inner_join(town, by="Agencia_ID")

ggplot(agencias.history %>% filter(Agencia_ID %in% top30agencias))+
  geom_bar(aes(x=Semana, y=Units, fill=Return_Rate), stat="identity", color="black")+
  facet_wrap(~Agencia_ID)+
  scale_y_continuous(labels=function(x)paste(x/1000, "k"))+
  scale_fill_gradient(name="Units\nReturn %", low="white", high="red")+
  ggtitle("Top 30 agencias")+
  theme_bw()

states <- agencias.history %>%
  group_by(State, Semana) %>%
  summarise(Units = sum(Units),
            Pesos = sum(Pesos),
            Return_Units = sum(Return_Units),
            Return_Pesos = sum(Return_Pesos),
            Net = sum(Net)) %>%
  mutate(Avg_Pesos = Pesos / Units,
         Return_Rate = Return_Units / (Units+Return_Units)) %>%
  arrange(desc(Units))

ggplot(states)+
  geom_bar(aes(x=Semana, y=Units, fill=Return_Rate), stat="identity", color="black")+
  scale_y_continuous(labels=function(x)paste(x/1e6, "m"))+
  scale_fill_gradient(name="Units\nReturn %", low="white", high="red")+
  facet_wrap(~State)+
  ggtitle("States")+
  theme_bw()
```

```{r echo=FALSE}
rm(states)
rm(agencias.history)
```

#Canals
Canal 1 is the most present.
```{r}
canals <- train %>%
  group_by(Canal_ID, Semana) %>%
  summarise(Units = sum(Venta_uni_hoy),
            Pesos = sum(Venta_hoy),
            Return_Units = sum(Dev_uni_proxima),
            Return_Pesos = sum(Dev_proxima),
            Net = sum(Demanda_uni_equil)) %>%
  mutate(Net_Pesos = Pesos - Return_Pesos,
         Avg_Pesos = Pesos / Units,
         Return_Rate = Return_Units / (Units+Return_Units)) %>%
  arrange(desc(Units))

treemap(canals, index=c("Canal_ID"), vSize="Units", type="index", title="Canals repartition")
  
ggplot(canals)+
  geom_bar(aes(x=Semana, y=Units, fill=Return_Rate), stat="identity", color="black")+
  scale_y_continuous(labels=function(x)paste(x/1e6, "m"))+
  scale_fill_gradient(name="Units\nReturn %", low="white", high="red")+
  facet_wrap(~Canal_ID, scale="free")+
  theme_bw()
```

```{r echo=FALSE}
rm(canals)
```

## Canals x Agencies
```{r}
agencias.canals <- train %>%
  group_by(Agencia_ID) %>%
  summarise(n_canals = n_distinct(Canal_ID))

ggplot(agencias.canals)+
  geom_histogram(aes(x=n_canals), fill="red", color="black", alpha="0.3", binwidth=0.5)+
  scale_x_continuous(name="Number of canals", breaks=1:5)+
  scale_y_continuous(name="Number of agencies")+
  theme(axis.text.x=element_text(hjust=1))+
  theme_bw()
```

```{r echo=FALSE}
rm(agencias.canals)
```

# Routes
It is not clear what Routes represent. More than 2/3 of the routes provide less than 10k products a week.
```{r}
routes <- train %>% group_by(Ruta_SAK) %>%
  summarise(n_Agencias = n_distinct(Agencia_ID),
            n_Clients = n_distinct(Cliente_ID),
            Units=sum(Venta_uni_hoy),
            Return_Units = sum(Dev_uni_proxima)) %>%
  mutate(Return_Rate = Return_Units / (Units+Return_Units)) %>%
  arrange(desc(Units))

ggplot(routes, aes(x=Units/7))+
  geom_histogram(fill="red", color="gray", binwidth=5000)+
  scale_x_continuous(name="Units / Week", labels=function(x)paste(x/1000, "k"))+
  scale_y_continuous(name="Routes")+
  theme_bw()
```

```{r echo=FALSE}
top100routes <- routes$Ruta_SAK[1:100]
rm(routes)
```

## Routes x Agencies
Even if there is no clear pattern, some routes seem to be working together with same agencies.
```{r}
routes.agencias <- train %>% group_by(Ruta_SAK, Agencia_ID) %>%
  summarise(count=n(),
            n_Clients = n_distinct(Cliente_ID),
            Units=sum(Venta_uni_hoy),
            Return_Units = sum(Dev_uni_proxima)) %>%
  mutate(Return_Rate = Return_Units / (Units+Return_Units)) %>%
  arrange(desc(Units))

ggplot(routes.agencias %>% 
         filter(Ruta_SAK %in% top100routes, Agencia_ID %in% top100agencias))+
  geom_point(aes(x=as.character(Ruta_SAK), 
                 y=as.character(Agencia_ID), 
                 size=Units, color=Return_Rate))+
  scale_x_discrete(name="Routes")+
  scale_y_discrete(name="Agencies")+
  scale_color_gradient(name="Return Rate", low="blue", high="red")+
  ggtitle("Top 100 agencies & routes")+
  theme_bw()+
  theme(axis.line=element_blank(),
        axis.text.x=element_blank(),
        axis.text.y=element_blank())
```
```{r echo=FALSE}
rm(routes.agencias)
```

# Clients
There is one big client, "Puebla Remision".
```{r, fig.height=8}
sales <- train %>%
  group_by(Cliente_ID) %>%
  summarise(Units = sum(Venta_uni_hoy),
            Pesos = sum(Venta_hoy),
            Return_Units = sum(Dev_uni_proxima),
            Return_Pesos = sum(Dev_proxima),
            Net = sum(Demanda_uni_equil)) %>%
  mutate(Return_Rate = Return_Units / (Units+Return_Units),
         Avg_Pesos = Pesos / Units) %>%
  mutate(Net_Pesos = Pesos - Return_Pesos) %>%
  inner_join(client, by="Cliente_ID") %>%
  arrange(desc(Pesos))

treemap(sales[1:100, ], 
        index=c("NombreCliente"), vSize="Units", vColor="Return_Rate", 
        palette=c("#FFFFFF","#FFFFFF","#FF0000"),
        type="value", title.legend="Units return %", title="Top 100 clients")
```
```{r}
sales$Cum_Units <- cumsum(sales$Units) / sum(sales$Units)
s <- seq(1, 800000, 100)
ggplot()+geom_line(aes(x=s, y=sales$Cum_Units[s]))+
  scale_x_continuous(name="Clients", labels=function(x) paste(x/1000, "k"))+
  scale_y_continuous(name="Cumulative share (units)", labels=percent)+
  ggtitle("Clients repartition")+
  theme_bw()

sales$share <- sales$Units / sum(sales$Units)
```

Herfindahl Index = `r round(1/sum(sales$share^2))` clients

```{r echo=FALSE}
rm(sales)
```

## Clients x Agencies
The large majority of clients only work with one agency. Only the largest clients work with multiple agencies.
```{r}
agencias.by.client <- train %>%
  group_by(Cliente_ID) %>%
  summarise(n_agencias = n_distinct(Agencia_ID)) %>%
  inner_join(client, by="Cliente_ID")

table(agencias.by.client$n_agencias)

agencias.by.client %>% filter(n_agencias %in% c(5, 9, 62))
```
```{r echo=FALSE}
rm(agencias.by.client)
```

## Clients x Canals
Most clients only have one canal. Different agencies can use the same canal for one client. 
```{r}
clients.canals <- train %>%
  group_by(Cliente_ID) %>%
  summarise(n_canals = n_distinct(Canal_ID))

table(clients.canals$n_canals)
```
```{r echo=FALSE}
rm(clients.canals)
```

Few agencies have multiple channels for the same client.
```{r}
clients.agencies.canals <- train %>%
  group_by(Cliente_ID, Agencia_ID) %>%
  summarise(n_canals = n_distinct(Canal_ID))

table(clients.agencies.canals$n_canals)
```
```{r echo=FALSE}
rm(clients.agencies.canals)
```

## Clients x Routes
Most client only have deliveries from less than 5 depots, but more than 2500 clients work with 10 depots or more.
```{r}
clients.routes <- train %>%
  group_by(Cliente_ID) %>%
  summarise(n_routes = n_distinct(Ruta_SAK))

ggplot(clients.routes)+
  geom_histogram(aes(x=n_routes), fill="red", color="black", alpha="0.3", binwidth=1)+
  scale_x_continuous(name="Number of clients")+
  scale_y_continuous(name="Number of routes", labels=function(x)paste(x/1000, "k"))+
  theme_bw()
```
```{r echo=FALSE}
rm(clients.routes)
```

# Products
```{r, fig.height=8}
products <- train %>% group_by(Producto_ID) %>%
  summarise(Units = sum(Venta_uni_hoy),
            Pesos = sum(Venta_hoy),
            Return_Units = sum(Dev_uni_proxima),
            Return_Pesos = sum(Dev_proxima),
            Net = sum(Demanda_uni_equil)) %>%
  mutate(Avg_Pesos = Pesos / Units,
         Return_Rate = Return_Units / (Units+Return_Units)) %>%
  filter(!is.nan(Avg_Pesos)) %>%
  inner_join(product, by="Producto_ID") %>%
  arrange(desc(Units))

products$NombreProducto <- factor(as.character(products$NombreProducto), levels=products$NombreProducto)

treemap(products[1:100, ], 
        index=c("NombreProducto"), vSize="Units", vColor="Return_Rate", 
        palette=c("#FFFFFF","#FFFFFF","#FF0000"),
        type="value", title.legend="Units return %", title="Top 100 products")
```
```{r}
ggplot(products, aes(x=Avg_Pesos))+
  geom_histogram(aes(y=..density..), fill="gray", color="black", alpha="0.3")+
  geom_density(fill="red", alpha="0.3")+
  scale_x_continuous(name="Products average price", lim=c(0, 50))+
  scale_y_continuous(name="Density", labels=percent)+
  theme_bw()
```
```{r echo=FALSE}
top100products <- products$Producto_ID[1:100]
rm(products)
```

## Products x Agencies
Agencies usually deal with between around 100 and 200 products.
```{r echo=FALSE}
agencias.products <- train %>% group_by(Agencia_ID) %>%
  summarise(n_products = n_distinct(Producto_ID))

ggplot(agencias.products)+
  geom_histogram(aes(x=n_products), fill="red", color="black", alpha="0.3", binwidth=10)+
  scale_x_continuous(name="Number of routes")+
  scale_y_continuous(name="Number of products")+
  theme_bw()
```
```{r echo=FALSE}
rm(agencias.products)
```

## Products x Canals
Products can be delivered through multiple channels.
```{r echo=FALSE}
canals.products <- train %>% group_by(Producto_ID) %>%
  summarise(n_canals = n_distinct(Canal_ID))

ggplot(canals.products)+
  geom_histogram(aes(x=n_canals), fill="red", color="black", alpha="0.3", binwidth=1)+
  scale_x_continuous(name="Number of canals", breaks=1:10, lim=c(1, 10))+
  scale_y_continuous(name="Number of products")+
  theme_bw()
```
```{r echo=FALSE}
rm(canals.products)
```

## Products x Routes
As expected, products can be in many depots, and depots can stock many products. 
However, it seems than some products are usually kept together.
```{r}
routes.products <- train %>% group_by(Producto_ID) %>%
  summarise(n_routes = n_distinct(Ruta_SAK))

ggplot(routes.products)+
  geom_histogram(aes(x=n_routes), fill="red", color="black", alpha="0.3", binwidth=10)+
  scale_x_continuous(name="Number of routes")+
  scale_y_continuous(name="Number of products")+
  theme_bw()

routes.products <- train %>% group_by(Ruta_SAK) %>%
  summarise(n_products = n_distinct(Producto_ID))

ggplot(routes.products)+
  geom_histogram(aes(x=n_products), fill="red", color="black", alpha="0.3", binwidth=10)+
  scale_x_continuous(name="Number of products")+
  scale_y_continuous(name="Number of routes")+
  theme_bw()

routes.products <- train %>% group_by(Ruta_SAK, Producto_ID) %>%
  summarise(count=n(),
            n_Agencias = n_distinct(Agencia_ID),
            n_Clients = n_distinct(Cliente_ID),
            Units=sum(Venta_uni_hoy),
            Return_Units = sum(Dev_uni_proxima)) %>%
  mutate(Return_Rate = Return_Units / (Units+Return_Units)) %>%
  arrange(desc(Units))

ggplot(routes.products %>% 
         filter(Ruta_SAK %in% top100routes, Producto_ID %in% top100products))+
  geom_point(aes(x=as.character(Ruta_SAK), 
                 y=as.character(Producto_ID), 
                 size=Units, color=Return_Rate))+
  scale_x_discrete(name="Ruta SAK")+
  scale_y_discrete(name="Product ID")+
  scale_color_gradient(name="Return Rate", low="blue", high="red")+
  ggtitle("Top 100 products & routes")+
  theme_bw()+
  theme(axis.line=element_blank(),
        axis.text.x=element_blank(),
        axis.text.y=element_blank())
```
```{r echo=FALSE}
rm(routes.products)
```

## Products x Clients
```{r}
products.by.client <- train %>%
  group_by(Cliente_ID) %>%
  summarise(n_products = n_distinct(Producto_ID)) %>%
  inner_join(client, by="Cliente_ID")

ggplot(products.by.client)+
  geom_histogram(aes(x=n_products), fill="red", color="black", alpha="0.3", binwidth=2)+
  scale_x_continuous(name="Number of products by client", lim=c(0, 150))+
  scale_y_continuous(name="Number of clients", labels=function(x)paste(x/1000, "k"))+
  theme_bw()
```
```{r echo=FALSE}
rm(products.by.client)
```





########### Method 2  #########################


## Version 1: all defaults for H2O GBM
## Version 2: fewer trees, row and column sample rate to restrict runtime
## Version 3: fixed mistake - inconsistent scoring and trees (15 v 20)
## Version 4: reduced memory limit back to original 6G from 8G
## Version 5: using product means as offset, dev/val/final splits to judge accuracy
## Version 6: reduced memory limit again to 6G (export into frame for submission uses extra RAM?)
## Version 7: write file directly from H2O (not pushing to R first); memory back to 7G
## Version 8: re-insert exp() on target (accidentally removed with transform to direct output

#################
## Set up Cluster (H2O is a Java ML Platform, with R/Python/Web/Java APIs)
#################
print(paste("Set up Cluster",Sys.time()))
library(h2o) # R API is just a library
## start a cluster; default is 2 cores for CRAN reasons; -1 = all
h2o.init(nthreads=-1,max_mem_size='7G')  

#############
## Load Data 
#############
print(paste("Load Data",Sys.time()))
## load the full training file, using all cores; load into H2O Frame known to R as train; H2O as train.hex
train<-h2o.uploadFile("../input/train.csv",destination_frame = "train.hex")

train[1:2,] ## take a look at a few rows
## train on the log of the target
train$target<-log(train$Demanda_uni_equil+1)

#################
## Partition Data
#################
print(paste("Partition Data",Sys.time()))
## This model will use three splits, partitioned by week: 
##   one to generate product averages, a second to fit a model, and a third to evaluate the model
dev<-train[train$Semana <= 5,]                    ## gets Semana 3,4,5
val<-train[train$Semana > 5 & train$Semana < 8,]  ## gets Semana 6, 7
val[1:5,]
final<-train[train$Semana == 8,]                  ## gets Semana 8
final[1:5,]

##############################
## Model: Product Groups & GBM
##############################
print(paste("Model: Product Groups & GBM",Sys.time()))
## use the fields available in test to predict; removing ID and Semana
##   for iteration, Semana would probably be a good field to use to control loops
##   in H2O you can directly use it as a "fold column" if you'd like
predictors<-c("Agencia_ID","Canal_ID","Ruta_SAK","Cliente_ID","Producto_ID")

## first part of model: use product averages, created on the dev set
##  this is the only time we will use the dev set
groups<-h2o.group_by(data=dev,by="Producto_ID",mean("target"))
groups[1:5,]

## apply groups back into dev and validation data sets as "mean_target"
## if there are NAs for this (new products), use a constant; used median of entire train target
newVal<-h2o.merge(x=val,y=groups,all.x = T)
newVal$mean_target[is.na(newVal$mean_target)]<-0.7
newVal[1:5,]
newFinal<-h2o.merge(x=final,y=groups,all.x = T)
newFinal$mean_target[is.na(newFinal$mean_target)]<-0.7
newFinal[1:5,]


## train a GBM; use aggressive parameters to keep overall runtime within 20 minutes
## this model is fit on Semana 6 & 7, and evaluated on Semana 8.
g<-h2o.gbm(
  training_frame = newVal,      ## H2O frame holding the training data
  validation_frame = newFinal,  ## extra holdout piece for three layer modeling
  x=predictors,                 ## this can be names or column numbers
  y="target",                   ## target: using the logged variable created earlier
  model_id="gbm1",              ## internal H2O name for model
  ntrees = 15,                  ## use fewer trees than default (50) to speed up training
  learn_rate = 0.3,             ## lower learn_rate is better, but use high rate to offset few trees
  score_tree_interval = 3,      ## score every 3 trees
  sample_rate = 0.5,            ## use half the rows each scoring round
  col_sample_rate = 0.8,        ## use 4/5 the columns to decide each split decision
  offset_column = "mean_target"
)

## look at model diagnostics
summary(g)

# clean up frames no longer needed
h2o.rm(train)
h2o.rm(dev)
h2o.rm(val)
h2o.rm(newVal)

#####################
## Create Predictions
#####################
print(paste("Create Predictions",Sys.time()))
## load test file
test<-h2o.uploadFile("../input/test.csv",destination_frame = "test.hex")
test[1:2,] ## take a look at a few rows of the test data
## merge in the offset column, just as with val and final
newTest<-h2o.merge(x=test,y=groups,all.x = T)
newTest$mean_target[is.na(newTest$mean_target)]<-0.7
newTest[1:5,]
p<-h2o.predict(g,newTest)
p<-exp(p)-1
summary(p)

####################
## Create Submission
####################
print(paste("Create Submission",Sys.time()))
submissionFrame<-h2o.cbind(test$id,p)
colnames(submissionFrame)<-c("id","Demanda_uni_equil")
h2o.exportFile(submissionFrame,path="h2o_gbm.csv")  ## export submission




### Method 3 ################


library(stringr)

preprocess_products <- function(product_data) {
  
  product_names <- as.character(product_data$NombreProducto)
  weight <- unlist(lapply(product_names, extract_weight))
  pieces <- unlist(lapply(product_names, extract_pieces))
  brand <- unlist(lapply(product_names, extract_brand))
  has_choco <- unlist(lapply(product_names, grepl, pattern="Choco"))
  has_vanilla <- unlist(lapply(product_names, grepl, pattern="Va(i)?nilla"))
  has_multigrain <- unlist(lapply(product_names, grepl, pattern="Multigrano"))
  
  data.frame(
    ID=product_data$Producto_ID,
    product_name=product_names,
    brand=brand,
    weight=weight,
    pieces=pieces,
    weight_per_piece=weight/pieces,
    has_choco=has_choco,
    has_vanilla=has_vanilla,
    has_multigrain=has_multigrain
  )
}

extract_token <- function(value, expr) {
  tokens <- strsplit(value, " ")
  index <- grep(expr, tokens)
  ifelse(length(index) == 0, NA, tokens[index[1]])
}

extract_weight <- function(product_name) {
  weight_str <- extract_token(product_name, "\\d+[Kg|g]")
  if (is.na(weight_str)) return(NA)
  groups <- str_match_all(weight_str, "(\\d+)(Kg|g)")
  weight <- strtoi(groups[[1]][2])
  unit <- groups[[1]][3]
  ifelse(unit == "Kg", 1000 * weight, weight)
}

extract_pieces <- function(product_name) {
  pieces_str <- extract_token(product_name, "\\d+p\\b")
  if (is.na(pieces_str)) return(NA)
  groups <- str_match_all(pieces_str, "(\\d+)(p)")
  return(strtoi(groups[[1]][2]))
}

extract_brand <- function(product_name) {
  tokens <- strsplit(product_name, " ")[[1]]
  tokens[length(tokens) - 1]
}

product_data <- read.csv('../input/producto_tabla.csv')
preprocessed <- preprocess_products(product_data)
write.csv(preprocessed, 'preprocessed_products.csv')



## Method 4 ########


###################################################################################################
#  
# Conversion of Abhishek Malali's Python script and concepts to R 
# https://www.kaggle.com/armalali/grupo-bimbo-inventory-demand/benchmark-medians/discussion
# 
# Scores 0.50758 on Public leaderboard
# Shows how R can work with large datsets at speed and within memory constraints
#
# Runs quickly on my home machine which has the benefit of SSD storage
#
#
###################################################################################################

#Load data.table for fast reads and aggreagtions
library(data.table)

# Input data files are available in the "../input/" directory.

# Read in only required columns and force to numeric to ensure that subsequent 
# aggregation when calculating medians works
train <- fread('../input/train.csv', 
               select = c('Cliente_ID', 'Producto_ID', 'Demanda_uni_equil'),
               colClasses=c(Cliente_ID="numeric",Producto_ID="numeric",Demanda_uni_equil="numeric"))

#Print first 6 rows and show that conversion from integer has been successful
head(train)
sapply(train, class)


# In case the pair of product/client pair has a median available, we use that as the predicted value.
# If not, then the product median is checked. If a value is not found, then the global median is used.

# set a table key to enable fast aggregations
setkey(train, Producto_ID, Cliente_ID)

#calculate the overall median
median <- train[, median(Demanda_uni_equil)]

#calculate the product overall median; call it M2
median_Prod <- train[, median(Demanda_uni_equil), by = Producto_ID]
setnames(median_Prod,"V1","M2")

#calculate the client and product  median; call it M3
median_Client_Prod <- train[, median(Demanda_uni_equil),by = .(Producto_ID,Cliente_ID)]
setnames(median_Client_Prod,"V1","M3")

###################################################################################################
#  
# That's the 'modeling' done now need to apply scoring to test set
# 
###################################################################################################

# Read in Test data 
# Read in only required columns and force to numeric

test <- fread('../input/test.csv', 
               select = c('id','Cliente_ID', 'Producto_ID'),
               colClasses=c(Cliente_ID="numeric",Producto_ID="numeric"))


#Print first 6 rows and show that conversion from integer has been successful
head(test)
sapply(test, class)

# set a table key to enable fast joins to predictions
setkey(test, Producto_ID, Cliente_ID)

# Create table called submit that joins medians (in field M3) by Product and Client to test data set
submit <- merge(test, median_Client_Prod, all.x = TRUE)

# add column M2 that contains median by Product
submit$M2 <- merge(test, median_Prod, by = "Producto_ID", all.x = TRUE)$M2


# Now create Predictions column; intially set to be M3 which contains median by product and client
submit$Pred <- submit$M3

# where median by product and client is null use median by product (M2)
submit[is.na(M3)]$Pred <- submit[is.na(M3)]$M2

# where median by product is null use overall median
submit[is.na(Pred)]$Pred <- median

# now relabel columns ready for creating submission
setnames(submit,"Pred","Demanda_uni_equil")

# check all looks OK
head(submit)

# Write out submission file.
# Any results you write to the current directory are saved as output.
write.csv(submit[,.(id,Demanda_uni_equil)],"submit.csv", row.names = FALSE)


##### Method 5 ################


# This R environment comes with all of CRAN preinstalled, as well as many other helpful packages
# The environment is defined by the kaggle/rstats docker image: https://github.com/kaggle/docker-rstats
# For example, here's several helpful packages to load in 

library(ggplot2) # Data visualization
library(readr) # CSV file I/O, e.g. the read_csv function

# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory

system("ls ../input")

# Any results you write to the current directory are saved as output.

rmsell <- function(predicted, actual) {
  sqrt(mean((log(predicted+1)-log(actual+1))^2))
}

tr <- read.csv('../input/train.csv', header=T, nrows=1)
colClasses <- rep('NULL', ncol(tr))
colClasses[colnames(tr) %in% c('Semana', 'Producto_ID', 'Demanda_uni_equil')] <- NA
train <- read.csv('../input/train.csv', header=T, colClasses=colClasses)

te <- read.csv('../input/test.csv', header=T, nrows=1)
colClasses <- rep('NULL', ncol(te))
colClasses[colnames(te) %in% c('Semana', 'Producto_ID', 'Demanda_uni_equil')] <- NA
test <- read.csv('../input/test.csv', header=T, colClasses=colClasses)


gc()
prod.name <- 'Producto_ID'

flag_test = T

if (flag_test) {
  products <- unique(test[, prod.name])
  test.n <- 1:nrow(test)
  target.product <- test[, prod.name]
  t <- train
} else {  
  products <- unique(train[, prod.name])
  test.n <- which(train[, 'Semana'] == 9)
  target.product <- train[test.n, prod.name]
  target.test.actual <- train[test.n, 'Demanda_uni_equil']
  t <- train[train[, 'Semana'] < 9, ]
}
target.test <- rep(0, length(test.n))
target.done <- rep(0, length(test.n))
library(doMC)
# Change to how many cores you have but for 8 cores needs 40gb memory, well there are better scripts...
registerDoMC(cores=1)


chunks.begin <- seq(1,length(products), 100)
chunks.end <- c(chunks.begin[2:length(chunks.begin)]-1, length(products))
for (j in 1:length(chunks.begin)) {
  x <- unlist(foreach(i = chunks.begin[j]:chunks.end[j]) %dopar% median(t[t[, prod.name] == products[i], 'Demanda_uni_equil']))
  x[is.na(x)] <- 6
  for (k in 1:length(x)) {
    target.test[target.product == products[chunks.begin[j]+k-1]] <- x[k]
    target.done[target.product == products[chunks.begin[j]+k-1]] <- 1
  }
  gc()
  if (flag_test) {
    writeLines(toString(chunks.end[j]))
  } else {
    writeLines(paste(toString(chunks.end[j]), toString(rmsell(target.test[target.done==1], target.test.actual[target.done==1]))))
  }
}

submission <- read.csv("../input/sample_submission.csv",header = TRUE, sep = ",")
submission[, 2] <- target.test
write.csv(submission, "submission_1.csv", row.names=F, quote=F)

### Method 6  #####

# This R environment comes with all of CRAN preinstalled, as well as many other helpful packages
# The environment is defined by the kaggle/rstats docker image: https://github.com/kaggle/docker-rstats
# For example, here's several helpful packages to load in 

library(ggplot2) # Data visualization
library(readr) # CSV file I/O, e.g. the read_csv function

# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory

system("ls ../input")

# Any results you write to the current directory are saved as output.

library(data.table)
library(ff)

#import train data
train <- fread("../input/train.csv", header = TRUE,nrow=120000)
#newdata=train[1:950000,]
#newdata=as.data.frame(newdata)

#import test data
test <- read.csv("../input/test.csv", header = TRUE)


#create model
library(gbm)
fit <- gbm(log(Demanda_uni_equil+1.1) ~ log(Semana+1.1) + log(Agencia_ID+1.1)+log(Canal_ID+1.1)+log(Ruta_SAK+1.1)+log(Cliente_ID+1.1)+log(Producto_ID+1.1),distribution="gaussian",data=train,n.trees=5,shrinkage=0.05,interaction.depth=4,bag.fraction=0.5,train.fraction=1,cv.folds=3,keep.data=TRUE,verbose=TRUE,n.cores=2)



best.iter<- gbm.perf(fit,method="cv")
#create prediction
prediction <- predict(fit, newdata = test, type="response",n.trees=best.iter)


str(prediction)


prediction[prediction<0] = 0.01  #threshold Demanda_uni_equil for negative ones are set to 0.01
prediction=exp(prediction)-1.1
#create submission file
submission2 <- data.frame(ID=test$id, Demanda_uni_equil=prediction)
write.csv(submission2, "submission2.csv", row.names = F)
library(MLmetrics)
r1=RMSLE(y_pred=prediction,y_true=train$Demanda_uni_equil)
print(r1)





################# Python ####################


import numpy as np

def run_solution():

    print ('')
    print ('Preparing Arrays...')
    print ('')

    f = open('../input/train.csv', "r")
    f.readline()

    latest_demand_clpro = dict()
    global_median = list()

    total = 0

    while 1:

        line = f.readline().strip()
        total += 1

        if total % 5000000 == 0:
            print('Read {} lines...'.format(total))

        if line == '':
            break

        arr = line.split(",")

        semana = int(arr[0])
        agencia = int(arr[1])
        canal = int(arr[2])
        ruta = int(arr[3])
        cliente = int(arr[4])
        producto = int(arr[5])
        demanda = int(arr[10])

        if cliente != '' and producto != '':
            hsh = (agencia, cliente, producto)
            if hsh in latest_demand_clpro:
                latest_demand_clpro[hsh] = ((.5 * latest_demand_clpro[hsh]) + (.5 * demanda))
            else:
                latest_demand_clpro[hsh] = demanda

        list.append(global_median, demanda)

    f.close()

    print ('')
    path = ('submission.csv')
    out = open(path, "w")
    f = open('../input/test.csv', "r")
    f.readline()

    out.write("id,Demanda_uni_equil\n")
    median_demanda = np.median(global_median)

    total = 0
    total1 = 0
    total2 = 0

    while 1:

        line = f.readline().strip()
        total += 1

        if total % 1000000 == 0:
            print('Write {} lines...'.format(total))

        if line == '':
            break

        arr = line.split(",")

        id = int(arr[0])
        semana = int(arr[1])
        agencia = int(arr[2])
        cliente = int(arr[5])
        producto = int(arr[6])

        out.write(str(id) + ',')

        hsh = (agencia, cliente, producto)
        if hsh in latest_demand_clpro:
            d = latest_demand_clpro[hsh]
            out.write(str(d))
            total1 += 1
        else:
            out.write(str(round(median_demanda)))
            total2 += 1

        out.write("\n")
    out.close()

    print ('')

    print ('Total 1: {} ...'.format(total1))
    print ('Total 2: {} ...'.format(total2))

    print ('')
    print ('Completed!')

run_solution()

# Any results you write to the current directory are saved as output.